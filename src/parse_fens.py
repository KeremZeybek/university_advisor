import os
import re
import json
from bs4 import BeautifulSoup

# --- AYARLAR ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_HTML_DIR = os.path.join(BASE_DIR, 'data', 'raw_html')
OUTPUT_FILE = os.path.join(BASE_DIR, 'data', 'json', 'fens_data_raw.json')

def clean_text(text):
    if not text: return ""
    return " ".join(text.replace('\xa0', ' ').split()).strip()

def is_course_code(text):
    # Regex: 2-5 harf, boÅŸluk, 3-4 rakam
    return bool(re.match(r"^[A-Z]{2,5}\s+\d{3,4}[A-Z]*$", text))

def parse_course_row(tr):
    cols = tr.find_all('td')
    if not cols: return None
    
    course_code = None
    course_name = ""
    ects = 0.0
    su_credit = 0.0
    
    # 1. Ders Kodunu Bul
    for idx, col in enumerate(cols):
        txt = clean_text(col.get_text())
        if is_course_code(txt):
            course_code = txt
            if idx + 1 < len(cols):
                course_name = clean_text(cols[idx+1].get_text())
            break
            
    if not course_code: return None

    # 2. Kredileri Bul
    nums = []
    for col in cols:
        txt = clean_text(col.get_text())
        if re.match(r"^\d+(\.\d+)?$", txt):
            nums.append(float(txt))
            
    if len(nums) >= 2:
        ects = nums[0]
        su_credit = nums[1]
    elif len(nums) == 1:
        su_credit = nums[0]

    return {
        "code": course_code,
        "name": course_name,
        "ects": ects,
        "su_credit": su_credit
    }

def find_courses_in_html(soup, section_keywords=None, forbidden_codes=None):
    """
    TablolarÄ± tarar. 
    forbidden_codes: EÄŸer tabloda bu kodlardan biri varsa, o tabloyu atla (YanlÄ±ÅŸ tabloyu almamak iÃ§in).
    """
    courses = []
    
    # Sayfadaki tÃ¼m tablolarÄ± al
    all_tables = soup.find_all('table')
    
    target_table = None
    
    # EÄŸer keyword varsa, o keyword'e en yakÄ±n tabloyu bulmaya Ã§alÄ±ÅŸ
    if section_keywords:
        # Ã–nce keywordleri iÃ§eren elementi bul
        header_node = None
        for kw in section_keywords:
            header_node = soup.find(string=re.compile(kw, re.IGNORECASE))
            if header_node: break
        
        if header_node:
            # O baÅŸlÄ±ktan sonra gelen tablolarÄ± incele
            current = header_node.find_parent()
            if current:
                next_tables = current.find_all_next('table')
                for tbl in next_tables:
                    # Tabloyu geÃ§ici parse et
                    temp_courses = []
                    rows = tbl.find_all('tr')
                    for tr in rows:
                        c = parse_course_row(tr)
                        if c: temp_courses.append(c['code'])
                    
                    # KONTROL: Bu tablo yasaklÄ± kod iÃ§eriyor mu?
                    # (Ã–rn: Required ararken AL 102 bulursan, bu Ãœniversite tablosudur, ATLA)
                    if forbidden_codes and any(fc in temp_courses for fc in forbidden_codes):
                        continue # Pas geÃ§, sonraki tabloya bak
                    
                    if temp_courses: # EÄŸer geÃ§erli ve yasaksÄ±z ders varsa
                        target_table = tbl
                        break # Bulduk!
    
    # EÄŸer spesifik hedef tablo yoksa veya bulunamadÄ±ysa (Pool dosyalarÄ± iÃ§in)
    tables_to_scan = [target_table] if target_table else all_tables
    
    for tbl in tables_to_scan:
        rows = tbl.find_all('tr')
        for tr in rows:
            course = parse_course_row(tr)
            if course:
                if not any(c['code'] == course['code'] for c in courses):
                    courses.append(course)
                    
    return courses

def main():
    print(f"ğŸ­ FENS Veri FabrikasÄ± (v4 - Anti-Overlap) Ã‡alÄ±ÅŸÄ±yor...\n")
    
    if not os.path.exists(RAW_HTML_DIR):
        print("âŒ HATA: raw_html klasÃ¶rÃ¼ bulunamadÄ±.")
        return

    all_majors = {}
    subdirs = [d for d in os.listdir(RAW_HTML_DIR) if os.path.isdir(os.path.join(RAW_HTML_DIR, d)) and d.endswith('_html')]
    
    for subdir in subdirs:
        major_code = subdir.replace('_html', '').upper()
        folder_path = os.path.join(RAW_HTML_DIR, subdir)
        print(f"   âš™ï¸  Ä°ÅŸleniyor: {major_code}...")
        
        prefix = subdir.replace('_html', '')
        files = {
            "main": os.path.join(folder_path, f"{prefix}_degree_detail.html"),
            "core": os.path.join(folder_path, f"{prefix}_core.html"),
            "area": os.path.join(folder_path, f"{prefix}_area.html"),
            "free": os.path.join(folder_path, f"{prefix}_free.html")
        }
        
        major_data = {
            "code": major_code,
            "requirements": {
                "university_courses": [],
                "required_courses": [],
                "core_electives": [],
                "area_electives": [],
                "free_electives": []
            }
        }
        
        if os.path.exists(files["main"]):
            with open(files["main"], "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f.read(), "html.parser")
                
            # 1. Ã–nce Ãœniversite Derslerini Ã‡ek
            uni_keys = ["University Courses", "Ãœniversite Dersleri"]
            major_data["requirements"]["university_courses"] = find_courses_in_html(soup, uni_keys)
            
            # Ãœniversite ders kodlarÄ±nÄ± bir listeye al (YasaklÄ± Liste)
            # Ã–rn: AL 102, CIP 101N, HIST 191...
            uni_codes = [c['code'] for c in major_data["requirements"]["university_courses"]]
            
            # 2. Åimdi Zorunlu Dersleri Ã‡ek (Ama YasaklÄ±larÄ± HariÃ§ Tut!)
            req_keys = ["Required Courses", "Major Required", "Zorunlu Dersler", "Program Requirements"]
            
            # EÄŸer "Required" diye ararken bulduÄŸu tabloda "AL 102" varsa, o tabloyu alma!
            major_data["requirements"]["required_courses"] = find_courses_in_html(
                soup, 
                req_keys, 
                forbidden_codes=["AL 102", "CIP 101N"] # Bu dersler varsa o tablo University tablosudur.
            )
            
        else:
            print(f"      âš ï¸ Ana dosya yok: {files['main']}")

        if os.path.exists(files["core"]):
            with open(files["core"], "r", encoding="utf-8") as f:
                major_data["requirements"]["core_electives"] = find_courses_in_html(BeautifulSoup(f.read(), "html.parser"))
        
        if os.path.exists(files["area"]):
            with open(files["area"], "r", encoding="utf-8") as f:
                major_data["requirements"]["area_electives"] = find_courses_in_html(BeautifulSoup(f.read(), "html.parser"))
                
        if os.path.exists(files["free"]):
            with open(files["free"], "r", encoding="utf-8") as f:
                major_data["requirements"]["free_electives"] = find_courses_in_html(BeautifulSoup(f.read(), "html.parser"))

        # Ã–zet
        c_req = len(major_data['requirements']['required_courses'])
        c_uni = len(major_data['requirements']['university_courses'])
        
        print(f"      ğŸ“Š Uni: {c_uni} | Req: {c_req}")
        all_majors[major_code] = major_data

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_majors, f, ensure_ascii=False, indent=4)
        
    print(f"\nğŸ‰ JSON DÃœZELTÄ°LDÄ°: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()